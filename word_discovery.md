###  新词发现论文 ：

公式编辑地址：https://latex.91maths.com/

 #### 专利新词发现的双向聚合度特征提取新方法

2020 计算机应用

+ 背景：针对专利新词具有通常具有较高复合度的特点，考虑构造一种基于双向条件概念的专利新词聚合度特征描述新方法

+ 框架

  专利文章  -> 二元词 - >双向条件概率 -> 双向聚合度 -> 词边界筛选规则  ->  新词

 + 双向条件概率
   
    + 二元词（bi-gram），分为 前向条件概率和后向条件概率
    
    + 前向条件概率
      $$
      p(w_j| w_i) =  \frac{f_{ij}}{f_{i}}
      $$
      
    +  后向条件概率
      $$
      p(w_i| w_j) =  \frac{f_{ij}}{f_{j}}
      $$
      
      $$
      其中 f_i 为 w_i 的频次，f_{ij} 为 w_i w_j 的频次。但通常地，首字 w_i 和尾字 w_j 并不相同，则 f_i 和 f_j 也有所不
      同，由此，首先引入二元词 的双向条件概率统计
      $$
      
    
 + 双向聚合度特征

    + 前向聚合度：
      $$
      ρ_→(w_i w_j) = p(w_j|w_i)× α_→ \frac{ln(f_{ij})}{d_{i→}}
      $$

      $$
      α_→= \frac {d_{max→}}{ln(f_{max} )}
      $$

      $$
      其中:α_→ 为如上式 的归一化因子，d_{i→} 为首字 w_i 的搭配数量，
      即有 d_{i→} 种具有相同尾字 w_i 的不同二元词。
      $$

      

    + 后向聚合度

      ​	
      $$
      ρ_←(w_i w_j)=p(w_i|w_j)× α_← \frac {ln(f_{ij})}{d_{j←}}
      $$

      $$
      α_←= \frac {d_{max←}}{ln(f_{max} )}
      $$

      
      $$
      其中:α_← 为如上式 的归一化因子，d_{j←} 为尾字 w_j 的搭配数量，
      即有 d_{j←} 种具有相同尾字 w_j 的不同二元词。
      $$
      

$$
其中:d_{max →} 为各首字搭配数量中的最大值，d_{max ←}为各尾字搭 配数量中的最大值，f_{max }为文档样本内二元词词频的最大值。
$$



 + 词边界筛选规则

    + 借鉴邻接熵思想，本文利用前、后向聚合度差值解析新词 词边界特征，设计提出可作用于新词候选项过滤筛选的词边 界规则。
      $$
      首先，将单个文字分为左边界文字、右边界文字以及非边 界文字。具体地，假设一字串为 w_i w_j w_k，则对于 w_j 有如下 3 种 情况:
      $$

    + 1) 当
      $$
      b(w_iw_j)- b(w_jw_k)≥ θ 时，w_j为右边界文字
      $$

    + 2) 当 
      $$
      b(w_iw_j)- b(w_jw_k)≤-θ时，w_j为左边界文字;
      $$

    + 3) 否 则
      $$
      ，w_j 为 非 边 界 文 字
      $$
其中θ为词边界筛选阈值，且对于任意$w_iw_j$$_，b(w_iw_j)的定义:
      b ( w_i w_j ) = min \left \{ ρ_→ ( w_i w_j ) ，ρ_← ( w_i w_j )\right \}$
      
      
    
+ 此外:若 $w_j$ 本身处于句首，则默认其为左边界文字;若本 身处于句尾，则默认其为右边界文字。
  
+ 最后，将词边界规则定义为:当候选项字串同时满足以下 条件 1)、2)时，保留此候选项，否则过滤此候选项:
      + 候选项字串的第一个文字为左边界文字
      + 候选项字串的最后一个文字为右边界文字。

#### 结合信息量和深度学习的领域新词发现

2019年计算机工程与设计

+ 背景：针对传统的新词发现中，数据的稀疏性使得一些低频词无法识别等问题，提出一种对分词结果计算信息量且将深度学习模型 bilstm-crf用于新词发现的方法，计算出信息量用以表示词语内部粘合度和分离度，并加入人工规则进行过滤

+ 框架：

  + 第一阶段

    + 对原始语料进行分词 -

    +  n-gram(1->4) 

    + 词频、互信息、左右熵 

    + 对每个词的4值进行阈值过滤，筛选出新词

    + 通过设置的规则，对新词再一次过滤，得到初步候选新词集

  + 第二阶段
    + 训练字向量数据，对原始语料进行分词，并且进行标注
    + 使用标注后的数据进行训练lilstm-crf模型得到 候选新词集
    + 汇总两个阶段的候选新词，取并集。完毕。



  

+ 相关公式：

  + 熵是对随机变了不确定性的度量，左右熵是指多字词表达的左边界的熵和右边界的熵
    $$
    H_L（x) =  - \sum_{ \forall  a \varepsilon A } P(a |x ) \log_2 P(a|x)
    $$

    $$
    H_R（x) =  - \sum_{ \forall  b \varepsilon B } P(b |x ) \log_2 P(b|x)
    $$

+ 实验对比：

  实验值对比 ： 把信息量作为baseline，然后对比信息量和hmm，信息量和crf，信息量和lstm-crf 、 信息量和bilstm-crf（本文）



#### 融合规则与统计的微博新词发现方法 

2017年 计算机应用

+ 背景:结合微博新词的构词规则自由度大和及其复杂的特点，针对传统的C/NC-value方法抽取的结果新词边界的识别准确率不高，以及低频微博新词无法正确识别的问题，提出一种融合人工启发式规则、C/NC-value改进算法和条件随机场（CRF）模型的微博新词抽取方法。

+ 框架

  +  微博语料 -> 统一编码格式->过滤部分特殊字符串->分词标注->分词标注语料

  ​      ->基于规则方法抽取->改进C/NC-value方法过滤->后处理->初步新词集->CRF模型训练->CRF标注识别 ->后处理-〉 输出新词词典

  

  + 微博语料 -> 统一编码格式->过滤部分特殊字符串->分词标注->分词标注语料->CRF标注识别 ->后处理-> 输出新词词典

+ 相关公式: NC-value

  + C-value

  $$
  C-value(\alpha ) = \begin{Bmatrix}
  \log_2|\alpha| \cdot f( \alpha ) &  \alpha  未必嵌套在其他候选词中  \\ 
  \log_2|\alpha| \cdot (f( \alpha ) - \frac{1}{P(T_a)}\sum_{\beta \varepsilon T_a} f(\beta)  ) &    其他
  \end{Bmatrix}
  $$

  ​	其中 $\alpha = \alpha_1 \alpha_2 \alpha_3 ....\alpha_n$ 是候选词; $｜\alpha|$ 表示长度;$f(\alpha)$ 表示 $\alpha$ 的词频；$T_{\alpha}$ 表示包含 $\alpha$ 的候选词集，$\beta$ 表示 $T_\alpha$ 中任意包含 $\alpha$ 的候选词;$f(\beta)$ 表示 $\beta$ 的词频， $P(T_\alpha)$ 表示包含 $\alpha$ 的候选词总数;

  > 公式的计算分为两种情况：
  > 首先，C-value方法是基于词串a的词频的。对于a的c-value的值计算，分为两种情况：
  > （1）a 不是嵌套串
  >           c-value的值就取决于a的频数和词串a的长度。算法认为，词串的字数对于词串的c-value  值起促进作用，换言之，词串越长，是术语的可能性就越大。
  > （2）a是嵌套串
  > 公式中 $\sum_{b \varepsilon T_a} f(b)$
  >
  > ​     表示的是包含a的长串b的词频，例如，a是”石油”，那么，b可能是“中石油”、”石油科技大学”，”西南石油”，”石油天然气”等等包含a的词串。该参数对词串的作用是消极的。即可以认为，一个词串a，若嵌套其的词串出现的频数较高，则a是术语的可能性就越小。例如，a是”石油”，那么f(b)为包含a的候选串”中石油”，f(b)出现的频数越高，表明f(b)是一个术语的可能性就越大，则a本身是一个术语的可能性就越小。
  >
  >    (Ta)表示的是所有含有词串a 的集合，例如，a是”石油”，那么，(Ta)就是“中石油”、”石油科技大学”，”西南石油”，”石油天然气”等等包含a的词串的集合。该参数对词串的作用是积极的。 P(Ta)表示(Ta)的个数，次数应该是4。(Ta)表征了a的独立性，若包含一个词串的集合个数越多，表明a在多个词中都出现过，则认为a有较强的独立性，更可能是一个术语。
  >
  > ​     总之，（1）一个词串a，若嵌套a的某个词串b出现的频数较高，则b是术语的可能性就较大，a是术语可能性就较小。（2）嵌套a的词串组成的集合越大，表明a在多个词串中以不同的形式出现，a的独立性就越高，越可能是术语。所以第一个参数对c-value的值起消极作用，第二个参数起积极作用。
  >
  > 另外还有一种 D-Value方法是一种基于术语词频[分布变化统计，](https://www.jianshu.com/p/5395b09915bf?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation)
  >
  > 

  + NC-value
    $$
    NC-value = \alpha * C-value(\alpha) + \beta \sum_{b \varepsilon C_\alpha} f_\alpha (b) * \frac{t(b)}{n}
    $$
    ​                                                                                              											$\alpha + \beta  =1$

    +  其中：$C_\alpha$表示出现在候选词 $\alpha$前后的上下文相关词集合，*b*表示$C_\alpha$中任意的出现在候选词前后的$\alpha$上下文相关词，$f_\alpha(b)$ 表示$b$在候选词$\alpha$的上下文中出现的次数，$t(b)$ 表示与$b$同时出现的候选词数量，$n$表示候选词的总个数。$α$和$β$为取值0~1的参数。

+ 改进的C/NC-value 算法：

  + 既有C/NC-value方法抽取微博新词的缺点主要包括：1) 部分识别结果存在词语粘连现象，新词的边界识别不正确；2) 低频新词无法正确识别。

  + 本文引入邻接熵和互信息两种统计量，重构NC-value目标函数，以提高新词边界的识别准确率和低频新词的识别精度。
    $$
    HL(\alpha)=−\sum p(x|\alpha) \log_2 p(x|\alpha)
    $$

    $$
    HR(\alpha)=−\sum_{y}^{x} p(x|\alpha) \log_2 p(x|\alpha)
    $$

    $$
    BE(\alpha)=min \begin{Bmatrix}  HL(\alpha),HR(\alpha) \end{Bmatrix}
    $$


  + 互信息是一个用来衡量候选词子串之间的结合程度的统计量。本文讲互信息加入到目标函数中，通过计算候选低频新词以及子串之间的结合程度来提高微博低频词的识别精度。互信息改进
    $$
    MI(w)= min_{1≤i≤n} log_{2}  \frac{p(w)}{p(w1…wi) p(wi+1…wn)}   
    $$

    + 其中：*p*(*w*) 表示$w$出现的频率；$p(w_1 w_2…w_i)$ 表示$w$的子串$w_1w_2…w_i$出现的频率；$p(w_{i+1}w_{i+2}…w_n)$ 表示$w$的子串$w_{i+1} w_{i+2}…w_{n}$出现的频率。改进后的NC-value值计算所示:
      $$
      NC−value(w)=α∗C−value(w)+β∗BE(w)+γ∗MI(w);α+β+γ=1
      $$

      + 其中：α、β和γ为参数，取值范围为[0, 1]

  + 条件随机场
    $$
    P(y|x)=\frac{1}{Z(X)} exp(\sum_{i}\sum_k λ_k t_k (y_{i−1},y_i,x,i)+\sum_i\sum_k u_ks_k(y_i,x,i))   
    $$
    

+ 实验对比

  + 3个基线系统方法（改进pmi+规则过滤，svm+规则过滤，有限状态+词法猜测）+ 单独规则+ 单独改进C/NC-value + 规则_传统C/NC-value + 规则和改进C/NC-value + 规则和改进C/NC-value和svm + 规则和改进C/NC-value+CRF

#### 基于改进互信息和邻接熵的微博新词发现方法

2016年计算机应用



