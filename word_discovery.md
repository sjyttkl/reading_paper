###  新词发现论文：
 #### 专利新词发现的双向聚合度特征提取新方法

2020 计算机应用

+ 背景：针对专利新词具有通常具有较高复合度的特点，考虑构造一种基于双向条件概念的专利新词聚合度特征描述新方法

+ 框架

  专利文章  -> 二元词 - >双向条件概率 -> 双向聚合度 -> 词边界筛选规则  ->  新词

 + 双向条件概率
   
    + 二元词（bi-gram），分为 前向条件概率和后向条件概率
    
    + 前向条件概率
      $$
      p(w_j| w_i) =  \frac{f_{ij}}{f_{i}}
      $$
      
    +  后向条件概率
      $$
      p(w_i| w_j) =  \frac{f_{ij}}{f_{j}}
      $$
      
      $$
      其中 f_i 为 w_i 的频次，f_{ij} 为 w_i w_j 的频次。但通常地，首字 w_i 和尾字 w_j 并不相同，则 f_i 和 f_j 也有所不
      同，由此，首先引入二元词 的双向条件概率统计
      $$
      
    
 + 双向聚合度特征

    + 前向聚合度：
      $$
      ρ_→(w_i w_j) = p(w_j|w_i)× α_→ \frac{ln(f_{ij})}{d_{i→}}
      $$

      $$
      α_→= \frac {d_{max→}}{ln(f_{max} )}
      $$

      $$
      其中:α_→ 为如上式 的归一化因子，d_{i→} 为首字 w_i 的搭配数量，
      即有 d_{i→} 种具有相同尾字 w_i 的不同二元词。
      $$

      

    + 后向聚合度

      ​	
      $$
      ρ_←(w_i w_j)=p(w_i|w_j)× α_← \frac {ln(f_{ij})}{d_{j←}}
      $$

      $$
      α_←= \frac {d_{max←}}{ln(f_{max} )}
      $$

      
      $$
      其中:α_← 为如上式 的归一化因子，d_{j←} 为尾字 w_j 的搭配数量，
      即有 d_{j←} 种具有相同尾字 w_j 的不同二元词。
      $$
      

$$
其中:d_{max →} 为各首字搭配数量中的最大值，d_{max ←}为各尾字搭 配数量中的最大值，f_{max }为文档样本内二元词词频的最大值。
$$



 + 词边界筛选规则

    + 借鉴邻接熵思想，本文利用前、后向聚合度差值解析新词 词边界特征，设计提出可作用于新词候选项过滤筛选的词边 界规则。
      $$
      首先，将单个文字分为左边界文字、右边界文字以及非边 界文字。具体地，假设一字串为 w_i w_j w_k，则对于 w_j 有如下 3 种 情况:
      $$

    + 1) 当
      $$
      b(w_iw_j)- b(w_jw_k)≥ θ 时，w_j为右边界文字
      $$

    + 2) 当 
      $$
      b(w_iw_j)- b(w_jw_k)≤-θ时，w_j为左边界文字;
      $$

    + 3) 否 则
      $$
       ，w_j 为 非 边 界 文 字
      $$

    + $$
      其中θ为词边界筛选阈值，且对于任意w_iwj_，b(w_iw_j)的定义:
      b ( w_i w_j ) = min \left \{ ρ_→ ( w_i w_j ) ，ρ_← ( w_i w_j )\right \}
      $$

      

    + 此外:若 wj 本身处于句首，则默认其为左边界文字;若本 身处于句尾，则默认其为右边界文字。
    + 最后，将词边界规则定义为:当候选项字串同时满足以下 条件 1)、2)时，保留此候选项，否则过滤此候选项:
      + 候选项字串的第一个文字为左边界文字
      + 候选项字串的最后一个文字为右边界文字。

#### 结合信息量和深度学习的领域新词发现

2019年计算机工程与设计

+ 背景：针对传统的新词发现中，数据的稀疏性使得一些低频词无法识别等问题，提出一种对分词结果计算信息量且将深度学习模型 bilstm-crf用于新词发现的方法，计算出信息量用以表示词语内部粘合度和分离度，并加入人工规则进行过滤

+ 框架：

  + 第一阶段

    + 对原始语料进行分词 -

    +  n-gram(1->4) 

    + 词频、互信息、左右熵 

    + 对每个词的4值进行阈值过滤，筛选出新词

    + 通过设置的规则，对新词再一次过滤，得到初步候选新词集

  + 第二阶段
    + 训练字向量数据，对原始语料进行分词，并且进行标注
    + 使用标注后的数据进行训练lilstm-crf模型得到 候选新词集
    + 汇总两个阶段的候选新词，取并集。完毕。



  

+ 相关公式：

  + 熵是对随机变了不确定性的度量，左右熵是指多字词表达的左边界的熵和右边界的熵
    $$
    H_L（x) =  - \sum_{ \forall  a \varepsilon A } P(a |x ) \log_2 P(a|x)
    $$

    $$
    H_R（x) =  - \sum_{ \forall  b \varepsilon B } P(b |x ) \log_2 P(b|x)
    $$

+ 实验对比：

  实验值对比 ： 把信息量作为baseline，然后对比信息量和hmm，信息量和crf，信息量和lstm-crf 、 信息量和bilstm-crf（本文）



#### 融合规则与统计的微博新词发现方法 

2017年 计算机应用

+ 背景
+ 框架
+ 相关公式
+ 实验对比